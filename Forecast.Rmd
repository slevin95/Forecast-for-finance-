---
title: "Forecast"
output: github_document
---

```{r setup, include=FALSE}
options(repos = "[http://cran.us.r-project.org](http://cran.us.r-project.org)") 
knitr::opts_chunk$set(echo = TRUE)
```

## Forecast

Goal and methods are to explore techniques of forecast in the field of financial markets and major drawbacks of such statistical methods.

## Code

The library used for the project are listed below:

-   library(dplyr)

-   library(fpp2)

-   library(fpp3)

-   library(ggplot2)

-   library(rugarch)

In the fpp2 is possible to find time-series for study purposes and the forecasting tools.

```{r include=FALSE}
install.packages("fpp2")
library(dplyr)
library(fpp2)
library(fpp3)
library(ggplot2)
```

## Exploratory analysis

Given the historical data provided by Yahoo Finance we get the last 5 years price of the popular S&P500. Studies can be done for many other securities. The repository include price quotation from May 19, 2019 - May 19, 2024 of the S&P500 and the Nasdq.

```{r pressure, echo=FALSE}
data<-read.csv("https://raw.githubusercontent.com/slevin95/Forecast-for-finance/main/SPX.csv")
head(data)
```

There are several reasons why adjusted prices are often preferred over raw prices for studying time series data with ARIMA models:

**1. Account for Corporate Actions:**

-   Raw stock prices can be affected by corporate actions like stock splits, dividends, and rights offerings. These events change the number of shares outstanding, impacting the raw price but not necessarily the underlying value of the company.

-   Adjusted prices incorporate these corporate actions, reflecting the true value of the stock over time. This is crucial for ARIMA models that analyze trends and patterns in the data.

**2. Improved Stationarity:**

-   ARIMA models perform best with stationary data, meaning the statistical properties (mean, variance) don't change significantly over time.

-   Corporate actions can introduce non-stationarity in raw prices due to sudden jumps or dips. Adjusted prices help mitigate this effect by smoothing out the impact of these events.

**3. Comparability:**

-   When analyzing price movements over extended periods, using adjusted prices ensures a more accurate comparison. This is because you're comparing the intrinsic value of the stock over time, not just the raw price fluctuations.

**4. Consistency with Financial Analysis:**

-   Financial analysts typically use adjusted prices for their calculations and valuations. By using adjusted prices in ARIMA models, you align your analysis with standard financial practices.

**Here are some additional points to consider:**

-   While adjusted prices are generally preferred, there might be specific research questions where raw prices are relevant (e.g., studying the immediate impact of a stock split).

-   The type of adjustment applied to the price will depend on the specific corporate action.

```{r}
dates=as.Date(data$Date)
prices=data$Adj.Close

plot(dates, prices, type="l", main="Adjusted Close price S&P500 in US $")
```

Is possible to identify a upwards trends that is not a characteristic of a time-serie with seasonality. Seasonal ts refers to recurring patterns within a specific time period, typically within a year (e.g.,monthly, quarterly). but seams impossible to detect seasonality.

```{r}
arima=auto.arima(prices, seasonal = FALSE)
summary(arima)

checkresiduals(arima)
```

-   **ARIMA(0,1,2):** This indicates an ARIMA model with:

    -   No autoregressive terms (AR=0), meaning the prediction doesn't rely on past values of the price series.

    -   A moving average term of order 1 (MA1), suggesting the model considers the previous error term (difference between predicted and actual value) in its forecast.

    -   A moving average term of order 2 (MA2), meaning it also takes into account the error term from two periods back.

-   **Drift:** The presence of drift signifies a long-term linear trend in the price series.

    ```{r}
    log_return= diff(log(prices))
    plot(log_return, type="l", main="Log returns S&P500")
    plot(forecast(arima, h=50))
    ```

With the transformations is possible to get a stationary ts which wasn't the case with the previous state of data.

Calculating the logarithm of the index returns can improve the study of the ts under the aspects of:

**Stationarity:** Financial time series data often exhibits non-stationarity, meaning the statistical properties (mean, variance) change over time. Log returns help achieve stationarity by focusing on the proportional changes in prices.

**Heteroskedasticity:** meaning the variance is not constant over time. Log returns can lead to more consistent variance and improving the performance of some models.

```{r}
acf(log_return)
acf(log_return^2)
plot(log_return, type="l", main="Log returns S&P500")
```

As shown in the the Auto-correlation function, the data distribution has an evident lag .

Log returns at the power of 2 can help highlighting volatility cluster over time as the returns are even more correlated with previous observation. As a matter of fact this findings violate the assumption of Arima models:

-   **Stationarity:** means that the mean, variance, and autocorrelation of the time series data are constant over time.

We try other models more suitable for our purpose.

```{r}
install.packages("rugarch")
library(rugarch)

gar= ugarchspec(variance.model = list(model="sGARCH",garchOrder=c(1,1)),
                                   mean.model = list(armaOrder=c(0,0)),
                                   distribution.model = "norm")
# fittiamo il modello
gar= ugarchfit(spec = gar,data = log_return)
gar
# sGARCH(1,1)
# grafici diagnostici
plot(gar, which=8)


## ripeto l'analisi con un modello con innovazioni t di student
# specifichiamo il modello
gar_t= ugarchspec(variance.model = list(model="sGARCH",garchOrder=c(1,1)),
                                   mean.model = list(armaOrder=c(0,0)),
                                   distribution.model = "std")
# fittiamo il modello
gar_t= ugarchfit(spec = gar_t, data = log_return)

# grafici diagnostici
plot(gar_t,which=8)
plot(gar_t@fit$z,type ="l")

## calcolo del Value at Risk al 99%
alpha = 0.99
## con gaussiana
VaR_gauss= -quantile(gar, probs = 1-alpha)
# plot(VaR_gauss)
plot(log_return,type = "l", main = "VaR99% GARCH(1,1)Gauss")+
lines(-as.numeric(VaR_gauss),col = "red")

# verifichiamo il numero di violazioni
mean(log_return < -VaR_gauss)

## con Student's t
VaR_t= -quantile(gar_t,probs = 1-alpha)
# plot(VaR_t)
plot(log_return,type = "l", main = "VaR99% GARCH(1,1)t")+
lines(-as.numeric(VaR_t),col = "red")
# verifichiamo il numero di violazioni


```

## **Implications of VaR with GARCH Models** 

GARCH (Generalized Autoregressive Conditional Heteroskedasticity) models are used to estimate the volatility of financial returns, which is crucial for accurate VaR calculations. Unlike standard volatility models, GARCH models account for the fact that volatility is often not constant over time but varies, exhibiting periods of high and low volatility.

VaR provides a quantifiable measure of potential loss in a portfolio, and incorporating GARCH models into VaR calculations allows for a more dynamic and realistic assessment of risk by accounting for time-varying volatility.

```{r}
#Value at Risk
a=0.05
quantile(log_return,a)
# This might become more accurate for risk management assessing with alpha=0.01

qplot(log_return , geom = 'histogram') + geom_histogram(fill='lightblue') +
geom_histogram(aes(log_return[log_return < quantile(log_return, a)]), fill='red') + labs(x = 'Daily Returns S&P500')

```

## Benchmarking Naïve

```{r}
naive=naive(prices, h = 20)  # h = 20 days
# Plot forecasts
autoplot(naive)
# Plot forecast with an xlim
autoplot(naive, include=200)
```

As for **Hewamalage et al.(2022) practitioners can check the goodness of a model benchmarking results with** Naïve **model, simple Random Walk equivalent to** ARIMA(0,1,0). 	

```{r}
library(fpp3)

data$Date <- as.Date(data$Date)
tsib= as_tsibble(data, index = Date)

tsib_filled <- tsib %>%
  fill_gaps() %>%
  mutate(Adj.Close = ifelse(is.na(Adj.Close), mean(Adj.Close, na.rm = TRUE), Adj.Close))


#fit=model(NAIVE(tsib$Adj.Close))
#autoplot(fit)
?NAIVE
```

## Forecast using Neural Networks

```{r}
fitnn=nnetar(prices, lambda = 0)
autoplot(forecast(fitnn, h=20, col="red"))

sim= ts(matrix(0, nrow=30L, ncol=9L),
start=end(prices)[1L]+1L)

for(i in seq(9))
  sim[,i]= simulate(fitnn, nsim=30L)
p= autoplot(ts(prices))
p + autolayer(sim, series = "Forecast")


fcast= forecast(fitnn, PI=TRUE, h=20)
autoplot(fcast)
```

Seems possible to try to model the time series of adjusted prices, with lambda=0 is performed a Box-Cox transformation. It's possible to create a cycle that iterate the forecast for several times and distribute the results plotting and graphically seeing the results.

\################

Figure 11.15: Forecasts with prediction intervals for the annual sunspot data. Prediction intervals are computed using simulated future sample paths.

Because it is a little slow, `PI=FALSE` is the default, so prediction intervals are not computed unless requested. The `npaths` argument in `forecast()` controls how many simulations are done (default 1000). By default, the errors are drawn from a normal distribution. The `bootstrap` argument allows the errors to be “bootstrapped” (i.e., randomly drawn from the historical errors)

\################

Sistemare appunti

```{r}

```
